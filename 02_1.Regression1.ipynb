{"cells":[{"cell_type":"markdown","metadata":{"id":"m03YIb1T6XE0"},"source":["# 実際に回帰問題を解いてみる"]},{"cell_type":"markdown","metadata":{"id":"ehJQ3dKC6XFA"},"source":["## 回帰問題編\n","\n","解説付きでコードを示しています．使い方としては\n","\n","1. 上から順に読んで（重要），\n","2. セルを実行していってください．\n","3. 理解できない場合は，スタッフに質問を投げるなどしてください．\n","4. 理解が進んだら，自分のノートを作成し，ページにコードを真似して記述していってください．"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":256,"status":"ok","timestamp":1659337504801,"user":{"displayName":"Shouno Hayaru","userId":"03549361908378438140"},"user_tz":-540},"id":"_VNAskmH6XFB"},"outputs":[],"source":["# まずは必要そうなものをインポート scipy は要らないかも\n","import numpy as np\n","import matplotlib.pylab as plt\n","import seaborn as sns; sns.set()"]},{"cell_type":"markdown","metadata":{"id":"MJhscxzS6XFC"},"source":["ここでは直線のモデル $y = a x + b$ を考えます．\n","\n","データは，直線モデルから，ノイズが加えられて観測したものと仮定し，人工的に生成します．\n","要は，機械学習が出すべき答えを知っている状態で，データを適用しどのような解が得られるのかを実験することが目的となります．\n","\n","詳細はコードに書いていますが，データを作成する場合は，直線モデルを具体的に与え(a = 1.2, b=0.5 などとして)，\n","ノイズの重畳過程も記載することが必要です．\n","ここではノイズの重畳は加法的ガウスノイズが重畳されるとし，ノイズの標準偏差は sgm = 0.2 としています．"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1659337505070,"user":{"displayName":"Shouno Hayaru","userId":"03549361908378438140"},"user_tz":-540},"id":"hoqsG74o6XFD"},"outputs":[],"source":["# ここらへんのパラメータを変えて遊んでみてください\n","\n","N = 50  #サンプル点の個数\n","a = 1.2  #モデル直線の傾き\n","b = 0.5  #モデル直線の切片\n","sgm = 0.2  # ノイズの標準偏差\n","\n","x = np.random.rand(N)    # [0, 1) の区間に N 個の乱数発生\n","ytrue = a * x + b\n","\n","y = ytrue  + sgm * np.random.randn(N)  # 真値にノイズを乗せて観測データを作る"]},{"cell_type":"markdown","metadata":{"id":"LJ3XJLpD6XFE"},"source":["### これで下準備完了\n","\n","一応，解説を入れておくと\n","\n","* `x`: データのx座標（乱数で生成） \n","* `ytrue`: a x + b 上の値 \n","* `y`: `ytrue` にガウスノイズを載せたもの \n","\n","です．\n","なので，(x, y) にデータが入っていると思いましょうという話です．\n","そこで，これらの点のプロットと真の直線の関係を見てみることにします．"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":286},"executionInfo":{"elapsed":453,"status":"ok","timestamp":1659337505519,"user":{"displayName":"Shouno Hayaru","userId":"03549361908378438140"},"user_tz":-540},"id":"P72g66q_6XFE","outputId":"480cbb9a-cdd5-48ba-e732-d0b028665230"},"outputs":[],"source":["# 散布図の描画\n","plt.figure()\n","plt.plot(x, y, 'bo', label='data')\n","plt.xlim(0, 1)\n","plt.ylim(0.4, 1.8)"]},{"cell_type":"markdown","metadata":{"id":"ZlNqcVzX6XFH"},"source":["このような青点の散布図が与えられたときに"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":286},"executionInfo":{"elapsed":596,"status":"ok","timestamp":1659337506107,"user":{"displayName":"Shouno Hayaru","userId":"03549361908378438140"},"user_tz":-540},"id":"Lw5vp_ey6XFH","outputId":"77240976-9850-43b7-c7b1-90b02e9024ec"},"outputs":[],"source":["# 真の関係を描画\n","plt.figure()\n","plt.plot(x, y, 'bo', label='data')\n","plt.xlim(0, 1)\n","plt.ylim(0.4, 1.8)\n","xx = np.linspace(0, 1, 128)\n","yy = a * xx + b\n","plt.plot(xx, yy, 'r-', label='true')\n","plt.legend()"]},{"cell_type":"markdown","metadata":{"id":"qWu0brFx6XFJ"},"source":["上図のような赤線を得ることができるのか？というのが問いかけになります．\n","少しかっこいい言葉遣いをすれば\n","\n","### 問い: データ点（青い点）のみから，もとになる直線（赤直線）を推定することは可能か？\n","\n","になります．これには解法としていくつか考えられますが，ここでは以下のような機械学習と呼ばれる手法に頼ることとします．"]},{"cell_type":"markdown","metadata":{"id":"jciO99Zc6XFK"},"source":["データの点群を $\\{(x_n, y_n)\\}$ とし， 推定モデルを $f(x; w) = w_1 x + w_0$ とおいて， $w_0$ と $w_1$ を推定することを考えます．\n","この問題は *最小二乗法* の問題で，　以下の関数（ロス関数と呼ばれる）\n","$$\n","    J(w) = \\frac{1}{N} \\sum_n (y_n - f(x_n))^2\n","$$\n","を，最小化する $w$ を求めます．\n","\n","### 要はモデル $f(x_n)$ と，　観測点 $y_n$ の差（残差）が小さくなる $w$ を求める．\n","\n","ことがやりたいことになります．\n","\n","この問題は，正規方程式\n","$$\n","    \\left(\\begin{array}{rr} N & \\sum x_n \\\\ \\sum x_n  & \\sum x_n^2 \\end{array}\\right) \n","    \\left(\\begin{array}{r} w_0 \\\\ w_1 \\end{array}\\right)\n","    =\n","     \\left(\\begin{array}{r} \\sum y_n \\\\ \\sum x_n y_n \\end{array}\\right)\n","$$\n","を解けばよい（導出が知りたければ質問すること）ことになります．\n","解は下記のとおりです．\n","$$\n","    \\left(\\begin{array}{r} w_0 \\\\ w_1 \\end{array}\\right)\n","    =\n","    \\frac{1}{N \\sum x_n^2 - (\\sum x_n)^2}\n","    \\left(\\begin{array}{rr} \\sum x_n^2  & - \\sum x_n \\\\ - \\sum x_n  & N\\end{array}\\right)\n","     \\left(\\begin{array}{r} \\sum y_n \\\\ \\sum x_n y_n \\end{array}\\right)\n","$$"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22,"status":"ok","timestamp":1659337506107,"user":{"displayName":"Shouno Hayaru","userId":"03549361908378438140"},"user_tz":-540},"id":"A86aVXbZ6XFL","outputId":"b4fc04d6-49d7-4f08-98cd-b220b079be04"},"outputs":[],"source":["# 最小二乗法から解をもとめよ\n","\n","#とりあえず統計量を計算しておく（上式の和記号が付いたやつ）\n","xsum = x.sum()\n","x2sum = (x**2).sum()\n","ysum = y.sum()\n","xysum = x @ y\n","\n","# 2x2 の逆行列は手計算で解ける\n","det = N * x2sum - (xsum)**2\n","w0 = (x2sum * ysum - xsum * xysum) / det\n","w1 = (-xsum * ysum + N * xysum) / det\n","\n","print(\"Estimate w1, w0 = ({:.3f}, {:.3f})\".format(w1, w0))\n","print(\"True      a,  b = ({:.3f}, {:.3f})\".format(a, b))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":286},"executionInfo":{"elapsed":20,"status":"ok","timestamp":1659337506107,"user":{"displayName":"Shouno Hayaru","userId":"03549361908378438140"},"user_tz":-540},"id":"aTVN24Ep6XFM","outputId":"cef43f47-d37f-4cbd-cc9a-78a56915cb3c"},"outputs":[],"source":["# 得られた直線が正しそうかプロット してみよう\n","\n","plt.plot(x, y, 'bo', label='data')\n","plt.xlim(0, 1)\n","plt.ylim(0.4, 1.8)\n","\n","xx = np.linspace(0, 1, 128)\n","yy = a * xx + b\n","plt.plot(xx, yy, 'r-', label='true')\n","yy = w1 * xx + w0\n","plt.plot(xx, yy, 'g-', label='estimate')\n","plt.legend() # 凡例を描画"]},{"cell_type":"markdown","metadata":{"id":"uxFco-3G6XFy"},"source":["緑のラインが推定で，赤のラインが真値．\n","\n","ついでに，残差とロス関数値も評価しておこう．　残差は モデル $f(x) = w_1 x + w_0$ が吐き出す予測値と $y$ の差の総和"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19,"status":"ok","timestamp":1659337506108,"user":{"displayName":"Shouno Hayaru","userId":"03549361908378438140"},"user_tz":-540},"id":"PyNtya2b6XFy","outputId":"761b4a64-4781-4e5e-9980-51b1d1675f24"},"outputs":[],"source":["# 残差を表示せよ\n","\n","residual = np.sum((y - (w1 * x + w0))**2)\n","print( \"Residual: {:.3f}\".format(residual))\n","print( \"Mean squared error {:.3f}\".format(residual/N))   #1点あたりのズレ（平均二乗誤差）"]},{"cell_type":"markdown","metadata":{"id":"GfyQykVT6XFz"},"source":["全体の残渣がおおよそ 2.5 なので，1点あたりのズレは 0.052 となり\n","\n","### うんだいたい良さげ\n","\n","ということがわかります．\n","\n","でも，正規方程式とか面倒です．．．で，この演習では python のパッケージを使った機械学習手法を習得しましょうという話になります．\n","紹介コードとしては，以下の２種を提供します．\n","\n","1. scikit-learn を使う\n","2. PyTorch を使う\n","\n","このレベルの問題であれば１を使うのが楽ですが，後半の画像認識では，選択肢２がメインになるので，一応両方とも解法を記載しておきます．\n","なお，モデルは一緒でも 1 は，上述の行列を用いて解きますが，2 は勾配法を用いて解きますので，解へのアプローチが異なります．"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# sklearn を使った解答\n","\n","from sklearn import linear_model   # 線形モデル導入\n","from sklearn.metrics import mean_squared_error  # 平均二乗誤差\n","\n","# モデルは，回帰(regression) なので regr というオブジェクトとして扱う\n","regr = linear_model.LinearRegression()\n","\n","#線形回帰モデル(Linear Regression()) が期待しているデータ形式は，各行にデータが入っているものなので reshape して渡す\n","regr.fit(x.reshape(N, 1), y.reshape(N, 1))"]},{"cell_type":"markdown","metadata":{},"source":["フィットさせることに成功した場合，なにもえらーは吐き出しません．\n","モデルを定義して, モデル内の fit() 関数を呼び出すだけでOKですが，知りたいのはモデルのパラメータなどです．\n","fit したあとは， モデル内の coef_ と intercept_ を見ればそこに解が入っています．"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# regression の coef_ と intercept_ は\n","# y = A x + b の A と b に対応．\n","# 成分でかくと y_i = \\sum_i A_ij x_j + b_i な感じ\n","# なので成分を出すためには coef_[i][j] とb[i] を指定する必要がある．\n","\n","print(\"w0, w1 = ({:.3f}, {:.3f})\".format(regr.coef_[0][0], regr.intercept_[0]))"]},{"cell_type":"markdown","metadata":{},"source":["## PyTorch を用いた解法\n","\n","PyTorch は，Facebook （現Meta）のもとで開発されてきた深層学習要の枠組み（フレームワーク）である．\n","（現在は Linux Foundation の下にいるっぽい）\n","深層学習の枠組みは様々なものがあったけど，Google の TensorFlow が死にそうなのでこちらを使って解いてみる．\n","\n","深層学習用のパッケージなので，普通は，\n","## こんな単純な回帰問題には *使わない* ．\n","けど理解するにはちょうど良いかもしれない．\n","\n","後半との連結をよくするために雰囲気で書いています．\n","（まじめに線形回帰するだけならscikit-learnのほうがうまく動きます）\n","\n","PyTorch でも\n","\n","1. 必要なオブジェクトのインポート\n","2. モデルの構築\n","3. fit（最適化）\n","\n","という手順の流れは一緒です．"]},{"cell_type":"markdown","metadata":{},"source":["まず，モデルとはなにかというところから考えます．\n","\n","上述の説明では，回帰モデルは \n","$$f(x; w) = w_1 x + w_0$$\n","です．パラメータは $w_1$ と $w_0$ な形で表されます．\n","モデルへの入力は $x$ で，出力が $f(x; w)$ となります．\n","\n","さて，これをニューラルネットワークで表現することを考えます．\n","ニューラルネットは，誤解を恐れずに言えば，丸（節点）と線（辺）でかく計算モデル（グラフ）です．\n","下図左の数式モデルを計算グラフで表すと右図のようになります．\n","\n","<img src=\"Images/modelLinear.png\" width=50%>\n","\n","入力 $x$ を持っている節点は出力ノード（オレンジ色のノードに）値$x$ を渡します．そのときに辺にかかっている\n","パラメータ($w_1$)を掛けて渡します．また１が入っている節点は１を出力するノードで，同じように辺の値($w_0$) を\n","かけて渡します．オレンジ色のノードは，渡された重み付きの入力を加算し，場合によっては非線形変換を行って，出力を出します．\n","この操作を各節点で順次行っていき最終的な出力を得ます．\n","このように入力を快層状に変換していく方法なので，階層型ニューラルネットワークと呼ばれます．\n","\n","ということで，この計算モデルをまず作ってみましょう．\n","PyTorch では，計算モデルはオブジェクトとして作成するのが一般的で，これは ``torch.nn.Module`` を継承することでのが一般的です．\n","クラス定義としては，最小限以下の２つのメソッドを用意します．\n","\n","* ``__init__()``: コンストラクタ．オブジェクトを生成する際に呼ばれる．ここで階層構造を作ります．\n","* ``forward()``: 順方向（入力から出力へ）の計算過程\n","\n","モデルとしては，入力 $x$ １個と，出力 $f(x; w)$ 1 個です．常に１という出力を出すユニットは 'バイアス項' と呼ばれ，入力変数としては扱わない場合が一般的です．\n","線形ユニットの記述は ``nn.Linear(出力個数, 入力個数, バイアス項の有無)`` で，\n","この設計を ``__init__()`` を埋め込みます．この場合 ``nn.Linear(1, 1, bias=True)`` となります．\n","\n","``forward()`` 計算では，階層構造に入力を入れて出てきた値を返す形でOKです．"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch\n","from torch import nn\n","\n","class LinearRegression(nn.Module):\n","    '''\n","    １入力１出力を行う線形回帰モデル\n","    '''\n","    def __init__(self):\n","        '''コンストラクタ'''\n","        super().__init__() # 継承元のコンストラクタを呼び出し\n","        # ネットワーク構造の記述，１入力１出力，バイアス項あり\n","        self.layer = nn.Linear(1, 1, bias=True) \n","    \n","    def forward(self, x):\n","        '''順方向の計算，入力xを出力y へ'''\n","        # 線形変換のみで記述\n","        y = self.layer(x)\n","        return y\n"]},{"cell_type":"markdown","metadata":{},"source":["モデルが決まったら，後は\n","* モデルのインスタンス化\n","* 損失関数を決めて\n","* 最適化を学習データに対して順次行う\n","ということをやります．モデルのインスタンス化は上記で定義した ``LinearRegression`` クラスを実体化することで行います．\n","損失関数は二乗誤差で定義しているので，``nn.MSEloss()`` で規定します．\n","最適化に関してはスタンダードな，確率勾配法を用います．\n","\n","勾配法がやっていることは，与えられた入力に対して微分をとって，その値が小さくなるように購買を下るように重みパラメータ $w_0, w_1$ を動かすことになります．"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# モデルをインスタンス化\n","model = LinearRegression()\n","\n","# 損失関数の定義\n","loss_criterion = nn.MSELoss()\n","\n","# 最適化手法\n","learning_rate = 0.01 # 学習率（1回にどのくらい更新するか）\n","moment_rate = 0.9    # 慣性項（1epoch 前の更新をどのくらい引きずるか）\n","optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=moment_rate)"]},{"cell_type":"markdown","metadata":{},"source":["頑張って，データに対してループを回していきます"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# epoch は何セット学習データを見せるか N 回データを通すと 1 epoch\n","\n","num_epoch = 1000\n","\n","# 入力は 32bit 浮動小数点を要求されるので ``torch.float32`` へ変換しておく\n","inputs = torch.from_numpy(x.reshape(-1, 1)).to(torch.float32)\n","targets = torch.from_numpy(y.reshape(-1, 1)).to(torch.float32)\n","\n","# ロスの記録用\n","hist = []\n","\n","for epoch in range(num_epoch):\n","    optimizer.zero_grad()\n","    outputs = model(inputs)\n","    loss = loss_criterion(outputs, targets) # ずれの量をはかる\n","\n","    # ズレからパラメータを更新\n","    loss.backward()\n","    optimizer.step()\n","\n","    hist.append(loss.item())\n","    # 更新してどのくらい良くなったかを表示（100回に１回程度の頻度で）\n","    if (epoch+1) % 100 == 0:\n","        print(f'Epoch[{epoch+1:02d}/{num_epoch:02d}]: loss = {loss.item():04f}')\n","\n","\n","# 結果の状態を保存したい場合は下をコメントアウトする．\n","# torch.save(model.state_dict(), 'model1.pickl')"]},{"cell_type":"markdown","metadata":{},"source":["うまくロス関数が小さくなっているかを確認する．"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# ロスの値が学習によってどう変わっていくのかを表示してみる\n","\n","plt.plot(np.array(hist))\n","#plt.semilogy(np.array(hist)) #対数スケールの方が収束したかは判断しやすい\n","plt.title('Loss Evolution')\n","plt.xlabel('Epochs')\n","plt.ylabel('loss')\n"]},{"cell_type":"markdown","metadata":{},"source":["かなり頑張って前出のロス値付近に収束しているぽい．\n","なので，パラメータを取り出して確認してみる"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["w1, w0 = model.layer.weight[0][0], model.layer.bias[0]\n","print( \"w0, w1 = ({:.3f}, {:.3f})\".format(w0, w1))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#w0 と w1 は， np.array なので，使いやすいように float 型へ変換しておく\n","w0 = float(w0)\n","w1 = float(w1)\n","\n","# あとはプロット\n","plt.plot(x, y, 'bo')\n","plt.xlim(0, 1)\n","plt.ylim(0.4, 1.8)\n","\n","xx = np.linspace(0, 1, 128)\n","yy = a * xx + b\n","plt.plot(xx, yy, 'r-')\n","yy = w1 * xx + w0\n","plt.plot(xx, yy, 'g-')"]},{"cell_type":"markdown","metadata":{},"source":["多分，前述の解とは微妙にずれているが，そんなに致命的ではないのを確認してください.\n","でも，`epoch` をもうちょっと増やして，頑張ると落とせるとは思います．\n","#### 逆説的に言えば，機械学習による推定とは，そのくらいの誤差が含まれるものと認識してください．"]}],"metadata":{"colab":{"name":"02_1.Regression1.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3.10.4 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.3"},"vscode":{"interpreter":{"hash":"949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"}}},"nbformat":4,"nbformat_minor":0}
